{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch._six'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpathlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtimm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m trunc_normal_\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpos_embed\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m interpolate_pos_embed\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\IHG6KOR\\Anaconda3\\envs\\mae_gan_model\\lib\\site-packages\\timm\\__init__.py:2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m create_model, list_models, is_model, list_modules, model_entrypoint, \\\n\u001b[0;32m      3\u001b[0m     is_scriptable, is_exportable, set_scriptable, set_exportable\n",
      "File \u001b[1;32mc:\\Users\\IHG6KOR\\Anaconda3\\envs\\mae_gan_model\\lib\\site-packages\\timm\\models\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcspnet\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdensenet\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdla\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\IHG6KOR\\Anaconda3\\envs\\mae_gan_model\\lib\\site-packages\\timm\\models\\cspnet.py:20\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtimm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\n\u001b[1;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhelpers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m build_model_with_cfg\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ClassifierHead, ConvBnAct, DropPath, create_attn, get_norm_act_layer\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mregistry\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m register_model\n",
      "File \u001b[1;32mc:\\Users\\IHG6KOR\\Anaconda3\\envs\\mae_gan_model\\lib\\site-packages\\timm\\models\\helpers.py:17\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_zoo\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmodel_zoo\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeatures\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FeatureListNet, FeatureDictNet, FeatureHookNet\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Conv2dSame, Linear\n\u001b[0;32m     20\u001b[0m _logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_state_dict\u001b[39m(checkpoint_path, use_ema\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "File \u001b[1;32mc:\\Users\\IHG6KOR\\Anaconda3\\envs\\mae_gan_model\\lib\\site-packages\\timm\\models\\layers\\__init__.py:7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mblur_pool\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BlurPool2d\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclassifier\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ClassifierHead, create_classifier\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcond_conv2d\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CondConv2d, get_condconv_initializer\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_exportable, is_scriptable, is_no_jit, set_exportable, set_scriptable, set_no_jit,\\\n\u001b[0;32m      9\u001b[0m     set_layer_config\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconv2d_same\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Conv2dSame\n",
      "File \u001b[1;32mc:\\Users\\IHG6KOR\\Anaconda3\\envs\\mae_gan_model\\lib\\site-packages\\timm\\models\\layers\\cond_conv2d.py:16\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nn \u001b[38;5;28;01mas\u001b[39;00m nn\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m functional \u001b[38;5;28;01mas\u001b[39;00m F\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhelpers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m to_2tuple\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconv2d_same\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m conv2d_same\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpadding\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_padding_value\n",
      "File \u001b[1;32mc:\\Users\\IHG6KOR\\Anaconda3\\envs\\mae_gan_model\\lib\\site-packages\\timm\\models\\layers\\helpers.py:6\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\" Layer/Module Helpers\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03mHacked together by / Copyright 2020 Ross Wightman\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mitertools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m repeat\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_six\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m container_abcs\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# From PyTorch internals\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_ntuple\u001b[39m(n):\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch._six'"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import datetime\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "from timm.models.layers import trunc_normal_\n",
    "from util.pos_embed import interpolate_pos_embed\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import timm\n",
    "assert timm.__version__ == \"0.3.2\"  # version check\n",
    "import timm.optim.optim_factory as optim_factory\n",
    "import util.misc as misc\n",
    "from util.misc import NativeScalerWithGradNormCount as NativeScaler\n",
    "import model_s_c_g_loss\n",
    "from engine_pretrain import train_one_epoch\n",
    "import os\n",
    "os.environ[\"TF_ENABLE_ONEDNN_OPTS\"] = \"0\"\n",
    "import prepare_data\n",
    "from functools import partial\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from timm.models.vision_transformer import PatchEmbed, Block\n",
    "from util.pos_embed import get_2d_sincos_pos_embed\n",
    "from matplotlib import pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm import tqdm\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "import torchvision.transforms as transforms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input to the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_args_parser():\n",
    "\n",
    "    parser = argparse.ArgumentParser('MAE pre-training', add_help=False)\n",
    "    parser.add_argument('--device', default='cuda:9',help='device to use for training / testing')\n",
    "    parser.add_argument('--batch_size', default=32, type=int,\n",
    "                        help='Batch size per GPU (effective batch size is batch_size * accum_iter * # gpus')\n",
    "    parser.add_argument('--epochs', default=100, type=int)\n",
    "    parser.add_argument('--model', default='mae_vit_large_patch16', type=str, metavar='MODEL',\n",
    "                        help='Name of model to train')\n",
    "    parser.add_argument('--src_img_path', default=r'/media/RTCIN7TBDriveB/home/ihg6kor/BDD_data/clear_daytime', type=str,\n",
    "                        help='source domain dataset path')\n",
    "    parser.add_argument('--target_img_path', default=r'/media/RTCIN7TBDriveB/home/ihg6kor/BDD_data/clear_night', type=str,\n",
    "                        help='target domain dataset path')\n",
    "    parser.add_argument('--finetune', default=r'/media/RTCIN7TBDriveB/home/ihg6kor/Domain_adaptation/mae_adaptation_idea2_0/preload_model/mae_visualize_vit_large_ganloss.pth',help='finetune from checkpoint')\n",
    "\n",
    "    \n",
    "    parser.add_argument('--input_size', default=224, type=int,\n",
    "                        help='images input size')\n",
    "\n",
    "    parser.add_argument('--mask_ratio', default=0.01, type=float,\n",
    "                        help='Masking ratio (percentage of removed patches).')\n",
    "    parser.add_argument('--lr', type=float, default=None, metavar='LR',\n",
    "                        help='learning rate (absolute lr)')\n",
    "    \n",
    "    parser.add_argument('--output_dir', default='./output_style_contentv2',\n",
    "                        help='path where to save, empty for no saving')\n",
    "    \n",
    "    parser.add_argument('--log_dir', default='./output_dir',\n",
    "                        help='path where to tensorboard log')\n",
    "    parser.add_argument('--seed', default=0, type=int)\n",
    "    parser.add_argument('--resume', default='',help='resume from checkpoint')\n",
    "\n",
    "    \n",
    "    parser.add_argument('--norm_pix_loss', action='store_true',\n",
    "                        help='Use (per-patch) normalized pixels as targets for computing loss')\n",
    "\n",
    "    return parser\n",
    "a = get_args_parser()\n",
    "args,unknown = a.parse_known_args()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device(args.device)\n",
    "    # simple augmentation\n",
    "transform_train = transforms.Compose([\n",
    "        transforms.CenterCrop(720),\n",
    "        transforms.Resize(args.input_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "\n",
    "        ])\n",
    "\n",
    "## loading the dataset\n",
    "\n",
    "dataset_src_domain = datasets.ImageFolder(args.src_img_path, transform=transform_train)\n",
    "dataset_target_domain = datasets.ImageFolder(args.target_img_path, transform=transform_train)\n",
    "\n",
    "## Dataloaders\n",
    "\n",
    "data_loader_src = torch.utils.data.DataLoader(\n",
    "    dataset_src_domain,\n",
    "    batch_size=args.batch_size,\n",
    "    drop_last=True,\n",
    "    shuffle = True\n",
    ")\n",
    "data_loader_target = torch.utils.data.DataLoader(\n",
    "    dataset_target_domain,\n",
    "    batch_size=args.batch_size,\n",
    "    drop_last=True,\n",
    "    shuffle = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_loader_src)*args.batch_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test for decoder\n",
    "# buf = model_GANLOSS.Discriminator()\n",
    "# out = buf(next(iter(data_loader_src))[0])\n",
    "# out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gen_model = model_s_c_g_loss.MaskedAutoencoderViT(\n",
    "        patch_size=16, embed_dim=1024, depth=24, num_heads=16,\n",
    "        decoder_embed_dim=512, decoder_depth=8, decoder_num_heads=16,\n",
    "        mlp_ratio=4, norm_layer=partial(nn.LayerNorm, eps=1e-6),norm_pix_loss=args.norm_pix_loss)\n",
    "\n",
    "disc_model = model_s_c_g_loss.Discriminator()\n",
    "\n",
    "def prepare_model(chkpt_dir, arch='mae_vit_large_patch16'):\n",
    "    # build model\n",
    "    model = gen_model\n",
    "    # load model\n",
    "    checkpoint = torch.load(chkpt_dir, map_location='cpu')\n",
    "    msg = model.load_state_dict(checkpoint['model'], strict=False)\n",
    "    print(msg)\n",
    "    return model\n",
    "chkpt_dir = args.finetune\n",
    "gen_model = prepare_model(chkpt_dir, 'mae_vit_large_patch16')\n",
    "print('Model loaded.')\n",
    "\n",
    "## verify weights inside the model\n",
    "for name, param in gen_model.named_parameters():\n",
    "    print(name, param.shape,torch.mean(param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Freeze the right layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for params in gen_model.parameters():\n",
    "    params.requires_grad  = False\n",
    "\n",
    "# for params in gen_model.transform_block.parameters():\n",
    "#     params.requires_grad  = True\n",
    "# for params in gen_model.transform_norm.parameters():\n",
    "#     params.requires_grad  = True\n",
    "\n",
    "for params in gen_model.decoder_embed.parameters():\n",
    "    params.requires_grad  = True\n",
    "for params in gen_model.decoder_blocks.parameters():\n",
    "    params.requires_grad  = True\n",
    "for params in gen_model.decoder_norm.parameters():\n",
    "    params.requires_grad  = True\n",
    "for params in gen_model.decoder_pred.parameters():\n",
    "    params.requires_grad  = True\n",
    "\n",
    "gen_model.to(device)\n",
    "disc_model.to(device)\n",
    "n_parameters = sum(p.numel() for p in gen_model.parameters() if p.requires_grad)\n",
    "print('number of params (M): %.2f' % (n_parameters / 1.e6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = model_s_c_g_loss.VGG().to(device).eval()\n",
    "\n",
    "def calc_content_loss(gen_image,src_image,loaded_model):\n",
    "    #calculating the content loss of each layer by calculating the MSE between the content and generated features and adding it to content loss\n",
    "    gen_features = loaded_model(gen_image)\n",
    "    src_features = loaded_model(src_image)\n",
    "    content_l = 0\n",
    "    # for i in range(len(gen_features)): \n",
    "    content_l=torch.mean((gen_features[-1]-src_features[-1])**2)\n",
    "    return content_l\n",
    "\n",
    "def calc_style_loss(gen_img,style_img,loaded_model):\n",
    "    #Calculating the gram matrix for the style and the generated image\n",
    "    style_all = []\n",
    "    gen_feat = loaded_model(gen_img)\n",
    "    style_feat = loaded_model(style_img)\n",
    "    \n",
    "    for feat in range(len(gen_feat)):\n",
    "        gen = gen_feat[feat]\n",
    "        style = style_feat[feat]\n",
    "        batch_size,channel,height,width=gen.shape\n",
    "        gen_calc = gen.view(batch_size,channel,height*width)\n",
    "        style_calc = style.view(batch_size,channel,height*width)\n",
    "        \n",
    "        G=gen_calc.bmm(gen_calc.transpose(1,2))\n",
    "        G /= channel * height * width\n",
    "        A=style_calc.bmm(style_calc.transpose(1,2))\n",
    "        A /= channel * height * width\n",
    "        style_all.append(torch.mean((G-A)**2))     \n",
    "    return torch.mean(torch.tensor(style_all))\n",
    "    \n",
    "def content_style_loss(src_image,gen_image,trg_image):\n",
    "    ## transform the input images \n",
    "    # src_image = torch,intransforms.Resize(size= (224,512))(src_image)\n",
    "    # gen_image = transforms.Resize(size= (512,512))(gen_image)\n",
    "    # trg_image = transforms.Resize(size= (512,512))(trg_image)\n",
    "    style_loss=content_loss=0\n",
    "    content_loss = calc_content_loss(gen_image,src_image,loaded_model)\n",
    "    style_loss = calc_style_loss(gen_image,trg_image,loaded_model)\n",
    "    return content_loss,style_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Testing the content and style loss\n",
    "\n",
    "# src_image = next(iter(data_loader_src))[0].to(device)\n",
    "# gen_image = next(iter(data_loader_src))[0].to(device)\n",
    "# trg_image = next(iter(data_loader_target))[0].to(device)\n",
    "# val = content_style_loss(1,10000,src_image , gen_image,trg_image)\n",
    "# print(\"total\",val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_mean_model(model):\n",
    "    layer_mean = []\n",
    "    for param in model.parameters(): \n",
    "        if param.requires_grad:\n",
    "            layer_mean.append(torch.mean(param))\n",
    "    return torch.mean(torch.tensor(layer_mean))\n",
    "\n",
    "def save_on_master(*args, **kwargs):\n",
    "    torch.save(*args, **kwargs)\n",
    "        \n",
    "def save_model(d_model,g_model,epoch,d_optim,g_optim):\n",
    "    output_dir = Path(args.output_dir)\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.mkdir(output_dir)\n",
    "    epoch_name = str(epoch)\n",
    "    checkpoint_d = [output_dir / ('disc_checkpoint-%s.pth' % epoch_name)]\n",
    "\n",
    "    to_save_d = {\n",
    "        'model': d_model.state_dict(),\n",
    "        'optimizer': d_optim.state_dict(),\n",
    "        'epoch': epoch,\n",
    "        'args': args\n",
    "    }\n",
    "\n",
    "    torch.save(to_save_d, checkpoint_d[0])\n",
    "    checkpoint_g = [output_dir / ('gen_checkpoint-%s.pth' % epoch_name)]\n",
    "    to_save_g = {\n",
    "    'model': g_model.state_dict(),\n",
    "    'optimizer': g_optim.state_dict(),\n",
    "    'epoch': epoch,\n",
    "    'args': args\n",
    "    }\n",
    "    torch.save(to_save_g, checkpoint_g[0])\n",
    "    print(\"saved models for epoch = \" , epoch)\n",
    "    return \n",
    "\n",
    "\n",
    "def show_images(img,label,epoch,args,iter_step):\n",
    "    epoch_name = str(epoch)\n",
    "    if not os.path.exists(args.output_dir):\n",
    "        os.mkdir(args.output_dir)\n",
    "    output_dir = Path(args.output_dir)/(\"images-epoch%s\"%epoch_name)\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.mkdir(output_dir)\n",
    "        \n",
    "    img = img.detach().cpu()\n",
    "    img = img[0]\n",
    "    imagenet_mean = np.array([0.485, 0.456, 0.406])\n",
    "    imagenet_std = np.array([0.229, 0.224, 0.225])\n",
    "\n",
    "    plt.figure(figsize = (10,2))\n",
    "    plt.imshow(torch.clip((img * imagenet_std + imagenet_mean) * 255, 0, 255).int())\n",
    "    loc = str(output_dir)+\"//\"+str(label)+\"_iter_\"+str(iter_step)+'.png'\n",
    "    plt.savefig(loc)    \n",
    "    matplotlib.pyplot.close()\n",
    "    return output_dir\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Losses defined\n",
    "criterion = nn.BCELoss()\n",
    "d_optimizer = torch.optim.Adam(disc_model.parameters(), lr=1e-3)\n",
    "g_optimizer = torch.optim.Adam(gen_model.parameters(), lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d_loss,d_fake,d_real = discriminator_train_step(args.batch_size,disc_model,gen_model,d_optimizer,criterion,target[:int(args.batch_size/2)],sample[:int(args.batch_size/2)])\n",
    "# \n",
    "def discriminator_train_step(batch_size, discriminator, generator, d_optimizer, criterion, real_images,src_images):\n",
    "    for params in discriminator.parameters():\n",
    "        params.requires_grad  = True\n",
    "    for params in generator.parameters():\n",
    "        params.requires_grad  = False\n",
    "    d_optimizer.zero_grad()\n",
    "    a = show_mean_model(generator)\n",
    "    b = show_mean_model(discriminator)\n",
    "    classify_real_image  = discriminator(real_images).view(-1).to(device)\n",
    "    # print(classify_real_image[1])\n",
    "    disc_real_loss = criterion(classify_real_image, torch.ones(classify_real_image.shape).to(device))\n",
    "    pred_img_latent,mask  = generator(src_images,real_images, mask_ratio=args.mask_ratio)\n",
    "    ## convert pred image to be same dimension as target image\n",
    "    pred =  generator.unpatchify(pred_img_latent)\n",
    "    classify_fake_image = discriminator(pred.detach()).view(-1).to(device)\n",
    "    disc_fake_loss = criterion(classify_fake_image, torch.zeros(classify_fake_image.shape).to(device))\n",
    "    d_loss = (disc_real_loss + disc_fake_loss)/2\n",
    "    d_loss.backward()\n",
    "    d_optimizer.step()\n",
    "    assert show_mean_model(generator) != a , \"generator paramters changing in Discriminator\"\n",
    "    # assert show_mean_model(discriminator) == b , \"discriminator paramters not in Discriminator\"\n",
    "    \n",
    "\n",
    "\n",
    "    return d_loss,disc_fake_loss,disc_real_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_train_step(loss_params,src_images,target_images, discriminator, generator, g_optimizer, criterion):\n",
    "    g_optimizer.zero_grad()\n",
    "    for params in discriminator.parameters():\n",
    "        params.requires_grad  = False\n",
    "    for params in gen_model.decoder_embed.parameters():\n",
    "        params.requires_grad  = True\n",
    "    for params in gen_model.decoder_blocks.parameters():\n",
    "        params.requires_grad  = True\n",
    "    for params in gen_model.decoder_norm.parameters():\n",
    "        params.requires_grad  = True\n",
    "    for params in gen_model.decoder_pred.parameters():\n",
    "        params.requires_grad  = True\n",
    "    a = show_mean_model(generator)\n",
    "    b = show_mean_model(discriminator)\n",
    "    pred_latent,mask = generator(src_images,target_images, mask_ratio=args.mask_ratio)\n",
    "    pred =  generator.unpatchify(pred_latent)\n",
    "    classify_fake_image = discriminator(pred).view(-1).to(device)\n",
    "    g_loss = criterion(classify_fake_image,torch.ones(classify_fake_image.shape).to(device))\n",
    "    content_loss,style_loss = content_style_loss(src_images,pred,target_images)\n",
    "\n",
    "    final_loss = loss_params[0]* g_loss + loss_params[1]*content_loss + loss_params[2] * style_loss\n",
    "    # print(\"final gen loss\",final_loss.item(),\"GAN_GEN_LOSS\",g_loss.item(), \"Content\",content_loss.item(), \"Style\", style_loss.item())\n",
    "    final_loss.backward()\n",
    "    g_optimizer.step()\n",
    "    # assert show_mean_model(generator) == a , \"generator paramters NOT changing in generator\"\n",
    "    assert show_mean_model(discriminator) != b , \"discriminator paramters CHANGING in generator\"\n",
    "    \n",
    "    return final_loss,pred,content_loss,style_loss,g_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##Start the training process\n",
    "\n",
    "alpha = 1\n",
    "beta = 1\n",
    "gamma = 100000\n",
    "\n",
    "\n",
    "print(f\"Start training for {args.epochs} epochs\")\n",
    "final_gloss = []\n",
    "final_dloss = []\n",
    "\n",
    "plt.ion()  \n",
    "# preparing the data\n",
    "x = [-3,-2,-1]\n",
    "d_fake_list= [0.1,0.6,0.7]\n",
    "d_real_list = [0.3,0.5,0.8]\n",
    "final_loss_list = [0.3,0.5,0.8]\n",
    "g_loss_list = [0.2,0.4,0.9]\n",
    "c_loss_list = [0.2,0.4,0.9]\n",
    "s_loss_list = [0.2,0.4,0.9]\n",
    "graph = plt.plot(x,d_fake_list,label = \"disc_fake\")[0]\n",
    "graph = plt.plot(x,d_real_list,label = \"disc_real\")[0]\n",
    "graph = plt.plot(x,final_loss_list,label = \"final_loss\")[0]\n",
    "graph = plt.plot(x,g_loss_list,label = \"gen_loss\")[0]\n",
    "graph = plt.plot(x,c_loss_list,label = \"content\")[0]\n",
    "graph = plt.plot(x,s_loss_list,label = \"style\")[0]\n",
    "plt.ylim(0,5)\n",
    "plt.legend()\n",
    "plt.pause(0.25)\n",
    "\n",
    "for epoch in range(0, args.epochs):    \n",
    "    gen_model.train(True)\n",
    "    iter_data_loader_target = iter(data_loader_target)\n",
    "    for iter_step,samples in enumerate(tqdm(data_loader_src)):\n",
    "        t,_= next(iter_data_loader_target)\n",
    "        target= t.to(device)\n",
    "        sample =samples[0].to(device)\n",
    "        final_loss,pred_img_unpatch,c_loss,s_loss,g_loss = generator_train_step([alpha,beta,gamma],sample[int(args.batch_size/2):],target[int(args.batch_size/2):],disc_model,gen_model,g_optimizer,criterion)\n",
    "        d_loss,d_fake,d_real = discriminator_train_step(args.batch_size,disc_model,gen_model,d_optimizer,criterion,target[:int(args.batch_size/2)],sample[:int(args.batch_size/2)])\n",
    "\n",
    "        if iter_step%20==0:\n",
    "            d_fake_list.append(d_fake.detach().cpu())\n",
    "            d_real_list.append(d_real.detach().cpu())\n",
    "            final_loss_list.append(final_loss.detach().cpu())\n",
    "            g_loss_list.append((g_loss*alpha).detach().cpu())\n",
    "            c_loss_list.append((c_loss*beta).detach().cpu())\n",
    "            s_loss_list.append((s_loss*gamma).detach().cpu())\n",
    "            x.append(epoch*len(data_loader_src)+iter_step)\n",
    "            \n",
    "            # print(\"epoch===\",epoch,\"discriminator loss=\",torch.mean(torch.tensor(dis_losses)),\" :: generator loss=\",torch.mean(torch.tensor(gen_losses)))\n",
    "            sample_print = torch.einsum('nchw->nhwc',sample[int(args.batch_size/2):])\n",
    "            target_print = torch.einsum('nchw->nhwc',target[int(args.batch_size/2):])\n",
    "            pred_img_unpatch = torch.einsum('nchw->nhwc',pred_img_unpatch)\n",
    "            out_loc = show_images(sample_print,\"source\",epoch,args,iter_step)\n",
    "            out_loc = show_images(target_print,\"target\",epoch,args,iter_step)\n",
    "            out_loc = show_images(pred_img_unpatch.detach().cpu(),\"generated\",epoch,args,iter_step)\n",
    "            loc = str(out_loc)+\"//losses_iter_\"+str(iter_step)+'.png'\n",
    "            ## Plotting the graphs\n",
    "            graph.remove()\n",
    "            graph = plt.plot(x,d_fake_list,color = 'g',label = \"disc_fake\")[0]\n",
    "            graph = plt.plot(x,d_real_list,color = 'r',label = \"disc_real\")[0]\n",
    "            graph = plt.plot(x,final_loss_list,color = 'k',label = \"final_loss\")[0]\n",
    "            graph = plt.plot(x,g_loss_list,color = 'b',label = \"gen_loss\")[0]\n",
    "            graph = plt.plot(x,c_loss_list,color = 'c',label = \"content\")[0]\n",
    "            graph = plt.plot(x,s_loss_list,color = 'm',label = \"style\")[0]\n",
    "            plt.xlim(x[0], x[-1])\n",
    "            # calling pause function for 0.25 seconds\n",
    "            plt.legend()\n",
    "            plt.savefig(loc)  \n",
    "            plt.pause(0.25)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final_gloss)\n",
    "print(final_dloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(disc_model,gen_model,100,d_optimizer,g_optimizer):\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
